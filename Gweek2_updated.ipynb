{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIER\n",
      "create dataframe from dictionary\n",
      "Normalizing data\n",
      "--------------------------------------\n",
      "Preparing binary classification task\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (test): 0.583\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json = 'data_folder'\n",
    "\n",
    "stop_list = list(STOPWORDS) + [\"sil\", \"uh\"]\n",
    "\n",
    "VOCAB = set()\n",
    "score_dict = {}\n",
    "tot = {}\n",
    "voc_dict = {}\n",
    "vocab_list = []\n",
    "\n",
    "\n",
    "new_dict = {}\n",
    "new_dict[\"f_1\"] = []\n",
    "new_dict[\"f_2\"] = []\n",
    "new_dict[\"f_3\"] = []\n",
    "new_dict[\"f_4\"] = []\n",
    "new_dict[\"scores\"] = []\n",
    "new_dict[\"binary_label\"] = []\n",
    "new_dict[\"multiclass_label\"] = []\n",
    "\n",
    "data = []\n",
    "\n",
    "setter = set()\n",
    "inter = set()\n",
    "# we need both the json and an index number so use enumerate()\n",
    "for subdir, dirs, files in os.walk(path_to_json):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            path = os.path.join(subdir, file)\n",
    "    \n",
    "            with open(path, 'r') as f:\n",
    "                json_text = json.load(f)\n",
    "            id_ = json_text[\"id\"]\n",
    "            feature_dict = {}\n",
    "\n",
    "            score = json_text[\"score\"]\n",
    "\n",
    "            if score < 5:\n",
    "                print(\"OUTLIER\")\n",
    "                continue\n",
    "\n",
    "            new_dict[\"scores\"].append(score)\n",
    "\n",
    "\n",
    "            text = \"\"\n",
    "            doc_vocab = set()\n",
    "            counter = 0\n",
    "            for tok in json_text[\"tokens\"]:\n",
    "                Text = tok[\"text\"].lower()\n",
    "                if Text not in stop_list:\n",
    "                    text += \" \" + Text\n",
    "                    counter += 1\n",
    "                    doc_vocab.add(Text)\n",
    "                    VOCAB.add(Text)\n",
    "                    vocab_list.append(Text)\n",
    "\n",
    "            data.append(text)\n",
    "\n",
    "\n",
    "#            for stop_word in stop_list:\n",
    "#                try:\n",
    "#                    doc_vocab.remove(stop_word)\n",
    "#                    VOCAB.remove(stop_word)\n",
    "#               except: continue\n",
    "    \n",
    "            # new words pr min\n",
    "            f_1 = len(doc_vocab)/json_text[\"elapsed_time\"]\n",
    "            if (f_1 < 0.15):\n",
    "                print('ERRORRRRRR')\n",
    "                print(subdir)\n",
    "\n",
    "\n",
    "            feature_dict[\"new_words_pr_min\"] = f_1\n",
    "\n",
    "\n",
    "            # repeated words pr min\n",
    "            f_2 = (counter - len(doc_vocab))/json_text[\"elapsed_time\"]\n",
    "            feature_dict[\"repeated_words_pr_min\"] = f_2\n",
    "            \n",
    "            new_dict[\"f_1\"].append(f_1)\n",
    "            new_dict[\"f_2\"].append(f_2)\n",
    "    \n",
    "            \n",
    "            feature_dict[\"time\"] = json_text[\"elapsed_time\"]\n",
    "            tot[id_] = feature_dict\n",
    "    \n",
    "            voc_dict[id_] = doc_vocab\n",
    "    \n",
    "            union = setter.union(doc_vocab-inter)\n",
    "            intersect = setter.intersection(doc_vocab)\n",
    "            setter = union - intersect\n",
    "            inter = intersect\n",
    "            \n",
    "\n",
    "            # Labelling process\n",
    "\n",
    "            # Goal: set thresholds to get uniform distribution\n",
    "            if score > 91:\n",
    "                new_dict[\"binary_label\"].append(1)\n",
    "                if score > 95.5:\n",
    "                    new_dict[\"multiclass_label\"].append(3)\n",
    "                else:\n",
    "                    new_dict[\"multiclass_label\"].append(2)\n",
    "            else:\n",
    "                new_dict[\"binary_label\"].append(0)\n",
    "                if (score < 91) & (score > 79):\n",
    "                    new_dict[\"multiclass_label\"].append(1)\n",
    "                else:\n",
    "                    new_dict[\"multiclass_label\"].append(0)\n",
    "\n",
    "\n",
    "# f_3\n",
    "for id_ in tot.keys():\n",
    "    voc = voc_dict[id_]\n",
    "    time = tot[id_][\"time\"]\n",
    "    f_3 = len(voc.intersection(setter))/(time)\n",
    "    new_dict[\"f_3\"].append(f_3)\n",
    "\n",
    "# f_4\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "#print(vectorizer)\n",
    "#print(X)\n",
    "transformer = TfidfTransformer()\n",
    "data = transformer.fit_transform(X).toarray()\n",
    "#print(data)\n",
    "\n",
    "summed_data = np.sum(data, axis=1)\n",
    "new_dict[\"f_4\"] = summed_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"create dataframe from dictionary\")\n",
    "df = pd.DataFrame.from_dict(new_dict)\n",
    "\n",
    "print(\"Normalizing data\")\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if \"f_\" in feature_name:\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "df = normalize(df)\n",
    "\n",
    "\n",
    "#print(df)\n",
    "#print(df)\n",
    "\n",
    "# print(\"Preprocessing done\")\n",
    "# print(\"Preparing classification task\")\n",
    "\n",
    "\n",
    "# print(\"Data Exploration\")\n",
    "\n",
    "# # Data exploration\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(\"Histograms\")\n",
    "\n",
    "# df[\"scores\"].hist(bins = 100)\n",
    "# plt.title(\"Gweek Score Histogram, 492 datapoints\")\n",
    "# plt.show()\n",
    "\n",
    "# df[\"binary_label\"].hist(bins = 2)\n",
    "# plt.title(\"Binary Label Histogram, 492 datapoints, (<95/>95)\")\n",
    "# plt.show()\n",
    "\n",
    "# df[\"multiclass_label\"].hist(bins = 4)\n",
    "# plt.title(\"Multiclass Label Histogram, 4 classes, 492 datapoints\")\n",
    "# plt.show()\n",
    "# #############\n",
    "# print(\"Correlation plots\")\n",
    "# plt.plot(df[\"f_1\"], df[\"scores\"], 'bo')\n",
    "# plt.xlabel('Distinct words pr sek')\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Correlation plot, 492 datapoints')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(df[\"f_1\"], df[\"f_3\"], 'bo')\n",
    "# plt.xlabel('Distinct words pr sek')\n",
    "# plt.ylabel('Special words')\n",
    "# plt.title('Correlation plot, 492 datapoints')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(df[\"f_1\"], df[\"f_4\"], 'bo')\n",
    "# plt.xlabel('Distinct words pr sek')\n",
    "# plt.ylabel('TF IDF')\n",
    "# plt.title('Correlation plot, 492 datapoints')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(df[\"f_4\"], df[\"scores\"], 'bo')\n",
    "# plt.xlabel('TF IDF')\n",
    "# plt.ylabel('Gweek Score')\n",
    "# plt.title('Correlation plot, 492 datapoints')\n",
    "# plt.show()\n",
    "\n",
    "# ###########Word Cloud###################\n",
    "\n",
    "# dicter = Counter(vocab_list)\n",
    "\n",
    "# wordcloud = WordCloud(\n",
    "#     background_color='white',\n",
    "#     stopwords=stop_list,\n",
    "#     max_words=200,\n",
    "#     max_font_size=80,\n",
    "#     random_state=42\n",
    "# ).generate_from_frequencies(dicter)\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.title(\"Word Cloud, 492 datapoints\")\n",
    "# plt.axis('off')\n",
    "# #plt.show()\n",
    "\n",
    "\n",
    "# # Training\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Preparing binary classification task\")\n",
    "\n",
    "y = df.iloc[:,-2]\n",
    "X = df.iloc[:,:-3]\n",
    "\n",
    "\n",
    "# # Training -------------------------------------\n",
    "# # splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#print(b_train)\n",
    "############Random Forest classifier#####################\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(a_train, b_train);\n",
    "# predictions = rf.predict(a_test)\n",
    "# #Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "# # Model Accuracy, how often is the classifier correct?\n",
    "# print(\"Accuracy for RandomForestClassifier:\",metrics.accuracy_score(b_test, predictions))\n",
    "\n",
    "# ######################## Naive Bayes classifier###############\n",
    "\n",
    "# #Import Gaussian Naive Bayes model\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# #Create a Gaussian Classifier\n",
    "# gnb = GaussianNB()\n",
    "\n",
    "# #Train the model using the training sets\n",
    "# gnb.fit(a_train, b_train)\n",
    "\n",
    "# #Predict the response for test dataset\n",
    "# prediction = gnb.predict(a_test)\n",
    "\n",
    "# from sklearn import metrics\n",
    "# # Model Accuracy, how often is the classifier correct?\n",
    "# print(\"Accuracy for Naive Bayes Classifier:\",metrics.accuracy_score(b_test, prediction))\n",
    "\n",
    "########################## Gradient Classifier###################\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(a_train, b_train)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(gb.score(a_train,b_train)))\n",
    "print(\"Accuracy score (test): {0:.3f}\".format(gb.score(a_test, b_test)))\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(a_train, b_train)\n",
    "# predicted = LR.predict(a_test)\n",
    "# print(\"F1-score BINARY\")\n",
    "# print(f1_score(predicted, np.array(b_test), average='weighted'))\n",
    "# #print(round(LR.score(predicted, np.array(b_test))))\n",
    "\n",
    "# print(\"--------------------------------------\")\n",
    "# print(\"Preparing multiclass classification task\")\n",
    "\n",
    "# y = df.iloc[:,-1]\n",
    "# #X = df.iloc[:,:-3]\n",
    "\n",
    "\n",
    "# # Training -------------------------------------\n",
    "# # splitting\n",
    "# a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(a_train, b_train)\n",
    "# predicted = LR.predict(a_test)\n",
    "# print(\"F1-score MULTICLASS\")\n",
    "# print(f1_score(predicted, np.array(b_test), average='weighted'))\n",
    "# #print(round(LR.score(predicted, np.array(b_test))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"--------------------------------------\")\n",
    "# print(\"Preparing regression task\")\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from math import sqrt\n",
    "\n",
    "# y = df.iloc[:,-3]\n",
    "\n",
    "# a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# LinR = LinearRegression().fit(a_train, b_train)\n",
    "# predicted = LinR.predict(a_test)\n",
    "# rmse = sqrt(mean_squared_error(predicted, b_test))\n",
    "# print(\"Root Mean square error\")\n",
    "# print(rmse)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
